{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Baseline Model.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"koAXqA6A9r1s","colab_type":"text"},"source":["---\n","\n","### Baseline Model\n","\n","\n","Now that we have done the pre-processing and decided the evaluation metric (RMSLE), we will create baseline models for forecasting demand. \n","\n","\n","Generally, we create baseline model using very basic techniques like mean prediction and then we try more complex solutions to improve the results that we got from the baseline model. The idea is to spot bugs in our final model as any score which is below baseline is not good enough.\n","\n","---\n","In this Notebook:\n"," - Predict the target using the mean demand from historical data for that particular store and product\n"," - Use Simple Moving Average (a very basic Time Series Model).\n"," - Train a linear Regression based model and Decision Tree model.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"kB6gNEyAxsKP","colab_type":"code","colab":{}},"source":["from "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBK6GCWv9r1u","colab_type":"code","colab":{}},"source":["# importing the required libraries\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.metrics import mean_squared_log_error as msle\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMJD6YHc9r1z","colab_type":"code","colab":{}},"source":["# reading the pre-processed dataset\n","train_data = pd.read_csv('updated_dataset/updated_train_data.csv')\n","product_data = pd.read_csv('updated_dataset/updated_product_data.csv')\n","store_data = pd.read_csv('updated_dataset/updated_store_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuxvmB1s9r12","colab_type":"code","colab":{},"outputId":"0511490b-09a0-4894-bc3a-1cbb62522514"},"source":["# view the train data\n","train_data.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WEEK_END_DATE</th>\n","      <th>STORE_NUM</th>\n","      <th>UPC</th>\n","      <th>BASE_PRICE</th>\n","      <th>FEATURE</th>\n","      <th>DISPLAY</th>\n","      <th>UNITS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14-Jan-09</td>\n","      <td>367</td>\n","      <td>1111009477</td>\n","      <td>1.57</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>14-Jan-09</td>\n","      <td>367</td>\n","      <td>1111009497</td>\n","      <td>1.39</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  WEEK_END_DATE  STORE_NUM         UPC  BASE_PRICE  FEATURE  DISPLAY  UNITS\n","0     14-Jan-09        367  1111009477        1.57        0        0     13\n","1     14-Jan-09        367  1111009497        1.39        0        0     20"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"gRXb9Rfh9r17","colab_type":"code","colab":{},"outputId":"b04f97cd-dc33-4ce5-da1d-33fe22950ff3"},"source":["# view the product data\n","product_data.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>UPC</th>\n","      <th>MANUFACTURER_1</th>\n","      <th>MANUFACTURER_2</th>\n","      <th>MANUFACTURER_3</th>\n","      <th>MANUFACTURER_4</th>\n","      <th>MANUFACTURER_5</th>\n","      <th>MANUFACTURER_6</th>\n","      <th>MANUFACTURER_7</th>\n","      <th>MANUFACTURER_8</th>\n","      <th>MANUFACTURER_9</th>\n","      <th>...</th>\n","      <th>CATEGORY_3</th>\n","      <th>CATEGORY_4</th>\n","      <th>SUB_CATEGORY_1</th>\n","      <th>SUB_CATEGORY_2</th>\n","      <th>SUB_CATEGORY_3</th>\n","      <th>SUB_CATEGORY_4</th>\n","      <th>SUB_CATEGORY_5</th>\n","      <th>SUB_CATEGORY_6</th>\n","      <th>SUB_CATEGORY_7</th>\n","      <th>PRODUCT_SIZE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1111009477</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1111009497</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 22 columns</p>\n","</div>"],"text/plain":["          UPC  MANUFACTURER_1  MANUFACTURER_2  MANUFACTURER_3  MANUFACTURER_4  \\\n","0  1111009477               1               0               0               0   \n","1  1111009497               1               0               0               0   \n","\n","   MANUFACTURER_5  MANUFACTURER_6  MANUFACTURER_7  MANUFACTURER_8  \\\n","0               0               0               0               0   \n","1               0               0               0               0   \n","\n","   MANUFACTURER_9  ...  CATEGORY_3  CATEGORY_4  SUB_CATEGORY_1  \\\n","0               0  ...           0           0               1   \n","1               0  ...           0           0               1   \n","\n","   SUB_CATEGORY_2  SUB_CATEGORY_3  SUB_CATEGORY_4  SUB_CATEGORY_5  \\\n","0               0               0               0               0   \n","1               0               0               0               0   \n","\n","   SUB_CATEGORY_6  SUB_CATEGORY_7  PRODUCT_SIZE  \n","0               0               0             2  \n","1               0               0             2  \n","\n","[2 rows x 22 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"KDGcAF219r1-","colab_type":"code","colab":{},"outputId":"9824bbdd-662c-413e-c6cd-2feaf19c87a1"},"source":["# view the store data\n","store_data.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>STORE_ID</th>\n","      <th>ADDRESS_STATE_PROV_CODE_1</th>\n","      <th>ADDRESS_STATE_PROV_CODE_2</th>\n","      <th>ADDRESS_STATE_PROV_CODE_3</th>\n","      <th>ADDRESS_STATE_PROV_CODE_4</th>\n","      <th>MSA_CODE_1</th>\n","      <th>MSA_CODE_2</th>\n","      <th>MSA_CODE_3</th>\n","      <th>MSA_CODE_4</th>\n","      <th>MSA_CODE_5</th>\n","      <th>MSA_CODE_6</th>\n","      <th>MSA_CODE_7</th>\n","      <th>MSA_CODE_8</th>\n","      <th>MSA_CODE_9</th>\n","      <th>SEG_VALUE_NAME</th>\n","      <th>SALES_AREA_SIZE_NUM</th>\n","      <th>AVG_WEEKLY_BASKETS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>367</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24721</td>\n","      <td>12707</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>389</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>46073</td>\n","      <td>24767</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   STORE_ID  ADDRESS_STATE_PROV_CODE_1  ADDRESS_STATE_PROV_CODE_2  \\\n","0       367                          1                          0   \n","1       389                          1                          0   \n","\n","   ADDRESS_STATE_PROV_CODE_3  ADDRESS_STATE_PROV_CODE_4  MSA_CODE_1  \\\n","0                          0                          0           1   \n","1                          0                          0           1   \n","\n","   MSA_CODE_2  MSA_CODE_3  MSA_CODE_4  MSA_CODE_5  MSA_CODE_6  MSA_CODE_7  \\\n","0           0           0           0           0           0           0   \n","1           0           0           0           0           0           0   \n","\n","   MSA_CODE_8  MSA_CODE_9  SEG_VALUE_NAME  SALES_AREA_SIZE_NUM  \\\n","0           0           0               1                24721   \n","1           0           0               2                46073   \n","\n","   AVG_WEEKLY_BASKETS  \n","0               12707  \n","1               24767  "]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"mJoloYP-9r2B","colab_type":"text"},"source":["### Merging Tables"]},{"cell_type":"code","metadata":{"id":"uKNFURKP9r2C","colab_type":"code","colab":{}},"source":["# merge the datasets\n","merge_data = train_data.merge(product_data, how='left', on= 'UPC')\n","merge_data = merge_data.merge(store_data, how= 'left', left_on= 'STORE_NUM', right_on= 'STORE_ID')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OocFB8Io9r2F","colab_type":"code","colab":{}},"source":["merge_data = merge_data.drop(columns=['STORE_ID'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFUj0kF89r2K","colab_type":"code","colab":{},"outputId":"1802fe9e-1b2e-43ce-ce71-8de05e910125"},"source":["# check if there is any null value in the final merged data set.\n","merge_data.isna().sum().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"pZS6tk_K9r2O","colab_type":"code","colab":{},"outputId":"64fa92a9-6c2b-49e4-c92d-7e09c05c05a4"},"source":["# let's look at a row, how data looks in the merged dataset\n","merge_data.loc[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["WEEK_END_DATE                 14-Jan-09\n","STORE_NUM                           367\n","UPC                          1111009477\n","BASE_PRICE                         1.57\n","FEATURE                               0\n","DISPLAY                               0\n","UNITS                                13\n","MANUFACTURER_1                        1\n","MANUFACTURER_2                        0\n","MANUFACTURER_3                        0\n","MANUFACTURER_4                        0\n","MANUFACTURER_5                        0\n","MANUFACTURER_6                        0\n","MANUFACTURER_7                        0\n","MANUFACTURER_8                        0\n","MANUFACTURER_9                        0\n","CATEGORY_1                            1\n","CATEGORY_2                            0\n","CATEGORY_3                            0\n","CATEGORY_4                            0\n","SUB_CATEGORY_1                        1\n","SUB_CATEGORY_2                        0\n","SUB_CATEGORY_3                        0\n","SUB_CATEGORY_4                        0\n","SUB_CATEGORY_5                        0\n","SUB_CATEGORY_6                        0\n","SUB_CATEGORY_7                        0\n","PRODUCT_SIZE                          2\n","ADDRESS_STATE_PROV_CODE_1             1\n","ADDRESS_STATE_PROV_CODE_2             0\n","ADDRESS_STATE_PROV_CODE_3             0\n","ADDRESS_STATE_PROV_CODE_4             0\n","MSA_CODE_1                            1\n","MSA_CODE_2                            0\n","MSA_CODE_3                            0\n","MSA_CODE_4                            0\n","MSA_CODE_5                            0\n","MSA_CODE_6                            0\n","MSA_CODE_7                            0\n","MSA_CODE_8                            0\n","MSA_CODE_9                            0\n","SEG_VALUE_NAME                        1\n","SALES_AREA_SIZE_NUM               24721\n","AVG_WEEKLY_BASKETS                12707\n","Name: 0, dtype: object"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"vwFx1rix9r2T","colab_type":"text"},"source":["### Creating the Validation Set"]},{"cell_type":"code","metadata":{"id":"ip03eS-_9r2T","colab_type":"code","colab":{}},"source":["# convert the column WEEK_END_DATE to datetime format\n","merge_data.WEEK_END_DATE = pd.to_datetime(merge_data.WEEK_END_DATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMrolIEw9r2X","colab_type":"text"},"source":["---\n","\n","- We will store the unique `WEEK_END_DATE` in a list so that it would be easier to split the data based on time. It will be used to create and train and validation split.\n","\n","- We will keep one week gap between the train and validation set and train set will start from the very begining from where the data is available.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"mWGVhzsF9r2Y","colab_type":"code","colab":{}},"source":["# store the unique dates as the dates are already sorted in the original dataframe, we can directly use the unique function\n","weeks = merge_data.WEEK_END_DATE.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDLAjZf29r2d","colab_type":"code","colab":{}},"source":["# define the function that will return a dictionary which contains the keys \n","\n","\"\"\"\n","[ \n","  {\n"," \"validation_set\" : ## validation set week,\n"," \"train_set_end_date\" : ## last week of trainind set with one gap with the validation set.\n","  },\n","  .\n","  .\n","  .\n","  {\n","   \"validation_set\" : ''\n","   \"train_set_end_date\" : ''\n","  }\n","]\n","\n","Initially, we will use the same start date of each training data set.\n","\n","\n","The function will take the parameter number which is the number of train and validation sets required.\n","\"\"\"\n","def get_train_validation_set(number=1):\n","    validation_sets = []\n","    for n in range(number):\n","        x = {}\n","        \n","        x['validation_set'] = weeks[len(weeks)-n-1]\n","        x['train_set_end_date'] = weeks[len(weeks)-n-3]\n","        validation_sets.append(x)\n","        \n","    return validation_sets\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbopp9GM9r2h","colab_type":"text"},"source":["We could go ahead with just 1 validation set to check the RMSLE score we are getting from different baseline models. However, taking multiple validation sets also allows us to look at the consistency of scores across multiple subsets of data.\n","\n","Here, we will take 5 validation sets for starters"]},{"cell_type":"code","metadata":{"id":"b0nYqpQW9r2h","colab_type":"code","colab":{}},"source":["# we will create our baseline model and test it on 5 different sets\n","validation_sets = get_train_validation_set(number=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUhU2ho59r2k","colab_type":"code","colab":{},"outputId":"6dbd587f-c0c7-4b0a-8c3b-5a1e8a7325f5"},"source":["# the dictionary that we got from our function \n","validation_sets"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'train_set_end_date': numpy.datetime64('2011-09-14T00:00:00.000000000'),\n","  'validation_set': numpy.datetime64('2011-09-28T00:00:00.000000000')},\n"," {'train_set_end_date': numpy.datetime64('2011-09-07T00:00:00.000000000'),\n","  'validation_set': numpy.datetime64('2011-09-21T00:00:00.000000000')},\n"," {'train_set_end_date': numpy.datetime64('2011-08-31T00:00:00.000000000'),\n","  'validation_set': numpy.datetime64('2011-09-14T00:00:00.000000000')},\n"," {'train_set_end_date': numpy.datetime64('2011-08-24T00:00:00.000000000'),\n","  'validation_set': numpy.datetime64('2011-09-07T00:00:00.000000000')},\n"," {'train_set_end_date': numpy.datetime64('2011-08-17T00:00:00.000000000'),\n","  'validation_set': numpy.datetime64('2011-08-31T00:00:00.000000000')}]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"RdYoUck59r2m","colab_type":"code","colab":{}},"source":["# Now, we will use that dictionary and store the train and validation sets as a list of tuples.\n","\n","\n","\"\"\"\n","data_set = [ (train_set_1, valid_set_1), ()....... (train_set_n, valid_set_n) ]\n","\n","\"\"\"\n","\n","data_set = []\n","\n","for data in validation_sets:\n","    \n","    training_data = merge_data[merge_data.WEEK_END_DATE <= data['train_set_end_date']]\n","    validation_data = merge_data[merge_data.WEEK_END_DATE == data['validation_set']]\n","    \n","    data_set.append((training_data, validation_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3lHPAUn9r2p","colab_type":"text"},"source":["---\n","\n","### MEAN PREDICTION\n","\n","\n","Now, we will create our first baseline model, `MEAN PREDICTION`. We will use the past data to take average on a group of `STORE_NUM` and `UPC` and use this to predict on the validaion set.\n","\n","\n","\n","\n","#### `Evaluation Metric: ` Root Mean Squared Log Error\n","---"]},{"cell_type":"code","metadata":{"id":"Hl-t7HHT9r2q","colab_type":"code","colab":{}},"source":["# define the function to get the RMSLE\n","\n","def get_msle(true, predicted) :\n","    return np.sqrt(msle( true, predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WU4M3nY9r2t","colab_type":"code","colab":{},"outputId":"d58a7871-c2bc-4c0d-edcf-3be93ff7ebb7"},"source":["train_rmsle = []\n","valid_rmsle = []\n","\n","for i, data in enumerate(data_set):\n","    \n","    # get the train and validation set\n","    train, valid = data\n","    \n","    # get the mean prediction dataframe by using a groupby on STORE_NUM and UPC\n","    mean_prediction = train.groupby(['STORE_NUM', 'UPC'])['UNITS'].mean().reset_index()\n","    \n","    # left join the train and validation set with the mean prediction.\n","    train = train.merge(mean_prediction, how='left', on=['STORE_NUM', 'UPC'])\n","    valid = valid.merge(mean_prediction, how='left', on=['STORE_NUM', 'UPC'])\n","    \n","    # In the updated dataframe after the left join, \n","    # column UNITS_x is the original value of the target variable\n","    # column UNITS_y is the predicted value of the target variable\n","    \n","    # get the rmsle on train and validation set\n","    t_rmsle = get_msle(train.UNITS_x, train.UNITS_y)\n","    v_rmsle = get_msle(valid.UNITS_x, valid.UNITS_y)\n","    train_rmsle.append(t_rmsle)\n","    valid_rmsle.append(v_rmsle)\n","    \n","    print('RMSLE ON TRAINING SET: ',i+1, ': ', t_rmsle)\n","    print('RMSLE ON VALIDATION SET: ',i+1, ': ',v_rmsle)\n","    print('=====================================================================')\n","    \n","# get the mean RMSLE on train and validation set.     \n","print('Mean RMSLE on Train: ', np.mean(train_rmsle))\n","print('Mean RMSLE on Valid: ', np.mean(valid_rmsle))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSLE ON TRAINING SET:  1 :  0.5902592110738579\n","RMSLE ON VALIDATION SET:  1 :  0.5887804241752373\n","=====================================================================\n","RMSLE ON TRAINING SET:  2 :  0.5912639141033686\n","RMSLE ON VALIDATION SET:  2 :  0.6263169979832923\n","=====================================================================\n","RMSLE ON TRAINING SET:  3 :  0.5917964778574165\n","RMSLE ON VALIDATION SET:  3 :  0.4783767090098456\n","=====================================================================\n","RMSLE ON TRAINING SET:  4 :  0.5914356263311358\n","RMSLE ON VALIDATION SET:  4 :  0.5811810487862565\n","=====================================================================\n","RMSLE ON TRAINING SET:  5 :  0.5916390592275275\n","RMSLE ON VALIDATION SET:  5 :  0.7181642414489362\n","=====================================================================\n","Mean RMSLE on Train:  0.5912788577186613\n","Mean RMSLE on Valid:  0.5985638842807136\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lqMZbAuO9r2x","colab_type":"text"},"source":["---\n","\n","### Simple Moving Average\n","\n","\n","- Now, we will use the Simple Moving Average, Like earlier we have used the average over the complete training period. Here, average will be taken on a specified period.\n","- We will use the predicted value as the average number of UNITS sold in last 8 weeks from a particular store of a particular product.\n","- As, we have one week gap between the train and validation set.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"P6KxTWPO9r2y","colab_type":"code","colab":{}},"source":["def get_sma(i, train, valid, no_of_weeks=2):\n","    \n","    # create a copy of train and validation set\n","    train_copy = train.copy()\n","    valid_copy = valid.copy()\n","    \n","    # group the data by STORE_NUM and UPC and use rolling and mean function to calculate the moving average.\n","    data_copy = train_copy.groupby(['STORE_NUM','UPC'])['UNITS'].rolling(no_of_weeks).mean().reset_index().set_index('level_2')\n","    \n","    # add the moving average column to the train data\n","    train_copy['moving_average'] = data_copy['UNITS']\n","    \n","    # the last prediction on train set will be used as prediction on validation set.\n","    # calculate the last_average dataframe by groupby using last function.\n","    last_average = train_copy.groupby(['STORE_NUM', 'UPC'])['moving_average'].last().reset_index()\n","    \n","    train_copy = train_copy[['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS', 'moving_average']]\n","    valid_copy = valid_copy[['WEEK_END_DATE','STORE_NUM', 'UPC', 'UNITS']]\n","    \n","    # drop the null values in the dataframe\n","    train_copy.dropna(inplace=True)\n","    # merge the validation data with the last_average by left join\n","    valid_copy = valid_copy.merge(last_average, how= 'left', on= ['STORE_NUM', 'UPC'])\n","    \n","    # calculate the rmsle on train and validation data\n","    t_rmsle = get_msle(train_copy['UNITS'], train_copy['moving_average'])\n","    v_rmsle = get_msle(valid_copy['UNITS'], valid_copy['moving_average'])\n","       \n","        \n","    print('RMSLE ON TRAINING SET: ',i+1, ': ', t_rmsle)\n","    print('RMSLE ON VALIDATION SET: ',i+1, ': ',v_rmsle)\n","    print('=====================================================================')\n","    \n","    return t_rmsle, v_rmsle\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXR7PD6s9r21","colab_type":"code","colab":{},"outputId":"1b3a813a-d143-4f52-a666-1272a6d01d93"},"source":["train_rmsle_ma = []\n","valid_rmsle_ma = []\n","\n","for i, data in enumerate(data_set):\n","    train, valid = data\n","\n","    t_rmsle, v_rmsle = get_sma(i,train, valid, no_of_weeks=8)\n","    train_rmsle_ma.append(t_rmsle)\n","    valid_rmsle_ma.append(v_rmsle)\n","    \n","print('Mean RMSLE on Train: ', np.mean(train_rmsle_ma))\n","print('Mean RMSLE on Valid: ', np.mean(valid_rmsle_ma))    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSLE ON TRAINING SET:  1 :  0.532302643351482\n","RMSLE ON VALIDATION SET:  1 :  0.5469206496913668\n","=====================================================================\n","RMSLE ON TRAINING SET:  2 :  0.5332435318948314\n","RMSLE ON VALIDATION SET:  2 :  0.6421015703332319\n","=====================================================================\n","RMSLE ON TRAINING SET:  3 :  0.5335704565359952\n","RMSLE ON VALIDATION SET:  3 :  0.46149085909723564\n","=====================================================================\n","RMSLE ON TRAINING SET:  4 :  0.5324889809038001\n","RMSLE ON VALIDATION SET:  4 :  0.5878031103068386\n","=====================================================================\n","RMSLE ON TRAINING SET:  5 :  0.5318770729185398\n","RMSLE ON VALIDATION SET:  5 :  0.7558881602487321\n","=====================================================================\n","Mean RMSLE on Train:  0.5326965371209298\n","Mean RMSLE on Valid:  0.598840869935481\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dGgdZTIn9r23","colab_type":"text"},"source":["---\n","\n","### Linear Regresssion\n","\n","\n","- Now, we will try one Linear Regression Model and see how it performs on our dataset. We will use the same 5 validation sets and compare the results. \n","- We will drop the columns like `WEEK_END_DATE`, `STORE_NUM` and `UPC` before training the model.\n","\n","\n","---"]},{"cell_type":"code","metadata":{"id":"ejv96TCG9r24","colab_type":"code","colab":{},"outputId":"057d8c66-4305-474b-879f-91d20a2b554c"},"source":["\n","train_rmsle_lr = []\n","valid_rmsle_lr = []\n","\n","for i, data in enumerate(data_set):\n","    \n","    train, valid = data\n","    \n","    # drop the columns that are not required, separate the target and independent features\n","    train_x = train.drop(columns=['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS'])\n","    train_y = train['UNITS']\n","    \n","    valid_x = valid.drop(columns=['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS'])\n","    valid_y = valid['UNITS']\n","    \n","    # create an Object of the Linear Regression model\n","    model_LR = LinearRegression(normalize=True)\n","    # fit the model with  the training data\n","    model_LR.fit(train_x, train_y)\n","    \n","    # predict on the training data \n","    # the model can predict some negative values also and RMSLE only supports positive values.\n","    # So, we will use the clip function. It will convert all the negative predicted values to 0.\n","    predict_train = model_LR.predict(train_x).clip(min=0)\n","    predict_valid = model_LR.predict(valid_x).clip(min=0)\n","    \n","    # get the rmsle on the training and validation data.\n","    t_rmsle = get_msle(train_y, predict_train)\n","    v_rmsle = get_msle(valid_y, predict_valid)\n","    train_rmsle_lr.append(t_rmsle)\n","    valid_rmsle_lr.append(v_rmsle)\n","    \n","    print('RMSLE ON TRAINING SET: ',i+1, ': ', t_rmsle)\n","    print('RMSLE ON VALIDATION SET: ',i+1, ': ',v_rmsle)\n","    print('=====================================================================')\n","    \n","    \n","    \n","print('Mean RMSLE on Train: ', np.mean(train_rmsle_lr))\n","print('Mean RMSLE on Valid: ', np.mean(valid_rmsle_lr))    \n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSLE ON TRAINING SET:  1 :  0.9955730328857578\n","RMSLE ON VALIDATION SET:  1 :  0.9181469786373301\n","=====================================================================\n","RMSLE ON TRAINING SET:  2 :  0.9900411764263702\n","RMSLE ON VALIDATION SET:  2 :  0.9007026223174186\n","=====================================================================\n","RMSLE ON TRAINING SET:  3 :  0.9899166439542934\n","RMSLE ON VALIDATION SET:  3 :  0.9580535987319388\n","=====================================================================\n","RMSLE ON TRAINING SET:  4 :  0.997040595263396\n","RMSLE ON VALIDATION SET:  4 :  0.9566364578664517\n","=====================================================================\n","RMSLE ON TRAINING SET:  5 :  0.9953956309251378\n","RMSLE ON VALIDATION SET:  5 :  0.9803209575040238\n","=====================================================================\n","Mean RMSLE on Train:  0.993593415890991\n","Mean RMSLE on Valid:  0.9427721230114325\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e5FE7pdD9r27","colab_type":"text"},"source":["---\n","\n","We can see that `Linear Regression` has performed really bad. Even predicting the mean vaues would be better model. So, it is clear the target variable has no linear dependency on the avaiable features.\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"7my2mfVN9r27","colab_type":"text"},"source":["---\n","\n","### Decision Tree\n","\n","- Now, we will try one Tree Based Model. We will use the same 5 validation sets and compare the results.\n","- We will drop the columns like `WEEK_END_DATE`, `STORE_NUM` and `UPC` before training the model.\n","\n","\n","\n","---"]},{"cell_type":"code","metadata":{"id":"qVXJzG-S9r28","colab_type":"code","colab":{},"outputId":"8bf39a5b-4e76-4c42-8530-7c9a73cebf45"},"source":["train_rmsle_dtr = []\n","valid_rmsle_dtr = []\n","\n","for i, data in enumerate(data_set):\n","    \n","    # get the train and validation set\n","    train, valid = data\n","    \n","    # drop the columns that are not required, separate the target and independent features    \n","    train_x = train.drop(columns=['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS'])\n","    train_y = train['UNITS']\n","    \n","    valid_x = valid.drop(columns=['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS'])\n","    valid_y = valid['UNITS']\n","    \n","    # create an Object of DecisionTree Regressor\n","    model_DTR = DecisionTreeRegressor()\n","    # fit the model with the training data\n","    model_DTR.fit(train_x, train_y)\n","    \n","    # predict the target and set the minimum value of the predicted target variable to be 0\n","    predict_train = model_DTR.predict(train_x).clip(min=0)\n","    predict_valid = model_DTR.predict(valid_x).clip(min=0)\n","    \n","    # get the rmsle on train and validation set.\n","    t_rmsle = get_msle(train_y, predict_train)\n","    v_rmsle = get_msle(valid_y, predict_valid)\n","    \n","    train_rmsle_dtr.append(t_rmsle)\n","    valid_rmsle_dtr.append(v_rmsle)\n","    \n","    print('RMSLE ON TRAINING SET: ',i+1, ': ', t_rmsle)\n","    print('RMSLE ON VALIDATION SET: ',i+1, ': ',v_rmsle)\n","    print('=====================================================================')\n","    \n","    \n","    \n","print('Mean RMSLE on Train: ', np.mean(train_rmsle_dtr))\n","print('Mean RMSLE on Valid: ', np.mean(valid_rmsle_dtr))    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSLE ON TRAINING SET:  1 :  0.41667563384749384\n","RMSLE ON VALIDATION SET:  1 :  0.45482383823139916\n","=====================================================================\n","RMSLE ON TRAINING SET:  2 :  0.41663996700336603\n","RMSLE ON VALIDATION SET:  2 :  0.4948556911025774\n","=====================================================================\n","RMSLE ON TRAINING SET:  3 :  0.4163555433255185\n","RMSLE ON VALIDATION SET:  3 :  0.4607170967921922\n","=====================================================================\n","RMSLE ON TRAINING SET:  4 :  0.4159250978004229\n","RMSLE ON VALIDATION SET:  4 :  0.5185064737606161\n","=====================================================================\n","RMSLE ON TRAINING SET:  5 :  0.4158593590916643\n","RMSLE ON VALIDATION SET:  5 :  0.5894785947522975\n","=====================================================================\n","Mean RMSLE on Train:  0.41629112021369313\n","Mean RMSLE on Valid:  0.5036763389278164\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y-mXOZD49r3A","colab_type":"text"},"source":["---\n","\n","So, we can see that Decision Tree performed way better than the LinearRegression and better than the Mean Prediction.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"PR5ihO1F9r3B","colab_type":"text"},"source":["---\n","\n","### RandomForest\n","\n","We just saw that the Decision Tree performed better than the Linear Regression Model. So, we will try one Ensemble Model of Decision Trees like RandomForest and compare the results on the same 5 validation sets.\n","\n","---"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"I81jfLpb9r3C","colab_type":"code","colab":{},"outputId":"99e6a1f9-a91d-41a0-b6bb-9cd4defcb920"},"source":["train_rmsle_rfr = []\n","valid_rmsle_rfr = []\n","\n","for i, data in enumerate(data_set):    \n","    # get the train and vaidation set\n","    train, valid = data\n","    \n","    # drop the columns that are not required, separate the target and independent features     \n","    train_x = train.drop(columns=['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS'])\n","    train_y = train['UNITS']\n","    \n","    valid_x = valid.drop(columns=['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS'])\n","    valid_y = valid['UNITS']\n","    \n","    # create an object of the Random Forest Regressor\n","    model_RFR = RandomForestRegressor(random_state=0)\n","    \n","    # fit the model with the training data\n","    model_RFR.fit(train_x, train_y)\n","    \n","    # predict the target and set the minimum value of the predicted target variable to be 0\n","    predict_train = model_RFR.predict(train_x).clip(min=0)\n","    predict_valid = model_RFR.predict(valid_x).clip(min=0)\n","    \n","    # get the rmsle on train and validate\n","    t_rmsle = get_msle(train_y, predict_train)\n","    v_rmsle = get_msle(valid_y, predict_valid)\n","    \n","    train_rmsle_rfr.append(t_rmsle)\n","    valid_rmsle_rfr.append(v_rmsle)\n","    \n","    print('RMSLE ON TRAINING SET: ',i+1, ': ', t_rmsle)\n","    print('RMSLE ON VALIDATION SET: ',i+1, ': ',v_rmsle)\n","    print('=====================================================================')\n","            \n","print('Mean RMSLE on Train: ', np.mean(train_rmsle_rfr))\n","print('Mean RMSLE on Valid: ', np.mean(valid_rmsle_rfr))    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSLE ON TRAINING SET:  1 :  0.4236803115086154\n","RMSLE ON VALIDATION SET:  1 :  0.4433437803932015\n","=====================================================================\n","RMSLE ON TRAINING SET:  2 :  0.4236990965056028\n","RMSLE ON VALIDATION SET:  2 :  0.4732457674446954\n","=====================================================================\n","RMSLE ON TRAINING SET:  3 :  0.42344642423851897\n","RMSLE ON VALIDATION SET:  3 :  0.4467805563728789\n","=====================================================================\n","RMSLE ON TRAINING SET:  4 :  0.42299034885722225\n","RMSLE ON VALIDATION SET:  4 :  0.5118645938373942\n","=====================================================================\n","RMSLE ON TRAINING SET:  5 :  0.42293393546982755\n","RMSLE ON VALIDATION SET:  5 :  0.5709839552059887\n","=====================================================================\n","Mean RMSLE on Train:  0.4233500233159574\n","Mean RMSLE on Valid:  0.48924373065083177\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kaDIQH7Y9r3I","colab_type":"text"},"source":["---\n","\n","## Conclusions\n","\n","- We have seen that Tree Based Models have a better performance than other models.\n","- We have a basic idea of what is the baseline.\n","- But still, we don't know what should be the right number of validation sets required and what should be the size of the training data.\n","- In the next notebook, we will try performance of model on different sizes of training period and different number of validation sets and choose the right validation startegy.\n","\n","\n","---"]}]}